# Claims Autoencoder System ğŸ¥

A production-ready anomaly detection system for insurance claims using deep learning autoencoders.

## ğŸ¯ Features

- **Advanced Architecture**: Configurable autoencoder with multiple hidden layers and dropout
- **Robust Data Pipeline**: Handles missing values, outliers, and mixed data types
- **MLflow Integration**: Complete experiment tracking and model versioning
- **Production Monitoring**: PSI (Population Stability Index) drift detection
- **Hyperparameter Tuning**: Optuna-based optimization
- **Interactive Dashboard**: Streamlit web interface for inference and monitoring
- **Batch Scoring**: Efficient processing of large datasets

## ğŸš€ Quick Start

### Installation

```bash
# Clone the repository
git clone <repository-url>
cd claims-autoencoder

# Create virtual environment
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt
```

### Configuration

Edit `config/example_config.yaml` to customize:
- Data paths and formats
- Model architecture
- Training parameters
- Feature engineering rules

### Training

```bash
python src/training.py --config config/example_config.yaml
```

### Batch Scoring

```bash
python src/batch_scoring.py \
    --model-path models/best_model.pth \
    --input-path data/claims_to_score.parquet \
    --output-path results/scored_claims.parquet
```

### Web Interface

```bash
streamlit run src/webapp.py
```

Access at `http://localhost:8501`

## ğŸ“ Project Structure

```
claims-autoencoder/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ config_manager.py       # Configuration handling
â”‚   â”œâ”€â”€ data_ingestion.py       # Data loading utilities
â”‚   â”œâ”€â”€ preprocessing.py        # Feature engineering
â”‚   â”œâ”€â”€ model_architecture.py   # Autoencoder model
â”‚   â”œâ”€â”€ training.py            # Training pipeline
â”‚   â”œâ”€â”€ evaluation.py          # Model evaluation
â”‚   â”œâ”€â”€ model_registry.py      # MLflow model management
â”‚   â”œâ”€â”€ batch_scoring.py       # Batch inference
â”‚   â”œâ”€â”€ psi_monitoring.py      # Drift detection
â”‚   â”œâ”€â”€ webapp.py              # Streamlit dashboard
â”‚   â””â”€â”€ hyperparameter_tuning.py  # Optuna tuning
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ conftest.py
â”‚   â””â”€â”€ test_*.py
â”œâ”€â”€ config/
â”‚   â””â”€â”€ example_config.yaml
â”œâ”€â”€ docs/
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

## ğŸ”§ Configuration

The system is highly configurable via YAML. Key sections:

```yaml
data:
  train_path: "data/claims_train.parquet"
  numerical_features: [...]
  categorical_features: [...]

model:
  encoding_dim: 32
  hidden_layers: [128, 64]
  dropout_rate: 0.3

training:
  batch_size: 256
  learning_rate: 0.001
  max_epochs: 100
```

## ğŸ“Š Model Architecture

The autoencoder uses:
- **Encoder**: Compresses input features to low-dimensional representation
- **Decoder**: Reconstructs original features from encoded representation
- **Loss Function**: MSE for reconstruction error (anomaly score)

Anomalies are detected when reconstruction error exceeds a threshold (typically 95th percentile).

## ğŸ” Monitoring

The system includes PSI monitoring to detect data drift:

```python
from src.psi_monitoring import PSIMonitor

monitor = PSIMonitor(reference_data, num_bins=10)
psi_scores = monitor.calculate_psi(new_data)
```

PSI thresholds:
- < 0.1: No significant change
- 0.1-0.2: Minor change
- \> 0.2: Major change (retrain recommended)

## ğŸ§ª Testing

```bash
# Run all tests
pytest tests/

# With coverage
pytest tests/ --cov=src --cov-report=html
```

## ğŸ›ï¸ Hyperparameter Tuning

```python
from src.hyperparameter_tuning import HyperparameterTuner

tuner = HyperparameterTuner(config, train_data, val_data)
best_params = tuner.optimize(n_trials=50)
```

## ğŸ“ˆ MLflow Tracking

View experiments:

```bash
mlflow ui
```

Access at `http://localhost:5000`

## ğŸ”’ Security

- Input validation and sanitization
- Secure file upload handling
- Rate limiting (1000 requests/day per user)
- No sensitive data in logs

## ğŸ¤ Contributing

1. Fork the repository
2. Create feature branch (`git checkout -b feature/amazing-feature`)
3. Commit changes (`git commit -m 'Add amazing feature'`)
4. Push to branch (`git push origin feature/amazing-feature`)
5. Open Pull Request

## ğŸ“ License

This project is licensed under the MIT License.

## ğŸ‘¥ Authors

- Your Name - Initial work

## ğŸ™ Acknowledgments

- PyTorch Lightning for training framework
- MLflow for experiment tracking
- Streamlit for web interface
- Optuna for hyperparameter optimization
