# Training Job Configuration
# Trains multiple fraud detection models in parallel with fairness validation

resources:
  jobs:
    model_training_job:
      name: "[${bundle.target}] Claims Fraud - Model Training"
      
      # Job configuration
      max_concurrent_runs: 1
      timeout_seconds: 7200  # 2 hours
      
      # Email notifications
      email_notifications:
        on_start:
          - ${var.notification_email}
        on_success:
          - ${var.notification_email}
        on_failure:
          - ${var.notification_email}
        on_duration_warning_threshold_exceeded:
          - ${var.notification_email}
      
      # Notification settings
      notification_settings:
        no_alert_for_skipped_runs: false
        no_alert_for_canceled_runs: false
      
      # Health monitoring
      health:
        rules:
          - metric: RUN_DURATION_SECONDS
            op: GREATER_THAN
            value: 7200  # Alert if exceeds 2 hours
      
      # Task orchestration
      tasks:
        # ========================================
        # Task 1: Data Ingestion & Validation
        # ========================================
        - task_key: data_ingestion
          job_cluster_key: data_processing_cluster
          
          libraries:
            - pypi:
                package: pandas>=2.0.0
            - pypi:
                package: pyarrow>=14.0.0
            - pypi:
                package: great-expectations>=0.18.0
          
          spark_python_task:
            python_file: src/data_ingestion.py
            parameters:
              - --catalog
              - ${var.catalog}
              - --schema
              - ${var.schema}
              - --output-table
              - ${var.catalog}.${var.schema}.training_data
              - --environment
              - ${bundle.target}
          
          timeout_seconds: 1800  # 30 minutes
        
        # ========================================
        # Task 2: Feature Engineering
        # ========================================
        - task_key: feature_engineering
          depends_on:
            - task_key: data_ingestion
          
          job_cluster_key: data_processing_cluster
          
          libraries:
            - pypi:
                package: scikit-learn>=1.3.0
          
          spark_python_task:
            python_file: src/feature_engineering.py
            parameters:
              - --catalog
              - ${var.catalog}
              - --schema
              - ${var.schema}
              - --input-table
              - ${var.catalog}.${var.schema}.training_data
              - --output-table
              - ${var.catalog}.${var.schema}.features
          
          timeout_seconds: 1200  # 20 minutes
        
        # ========================================
        # Task 3a: Train CatBoost Model (Parallel)
        # ========================================
        - task_key: train_catboost
          depends_on:
            - task_key: feature_engineering
          
          job_cluster_key: ml_training_cluster
          
          libraries:
            - pypi:
                package: catboost>=1.2.0
            - pypi:
                package: mlflow>=2.9.0
            - pypi:
                package: numpy>=1.24.0
            - pypi:
                package: pandas>=2.0.0
          
          spark_python_task:
            python_file: src/tree_models.py
            parameters:
              - --model-type
              - catboost
              - --catalog
              - ${var.catalog}
              - --schema
              - ${var.schema}
              - --experiment-name
              - ${var.experiment_path}/catboost
              - --run-name
              - catboost-${bundle.target}
          
          timeout_seconds: 3600  # 1 hour
        
        # ========================================
        # Task 3b: Train XGBoost Model (Parallel)
        # ========================================
        - task_key: train_xgboost
          depends_on:
            - task_key: feature_engineering
          
          job_cluster_key: ml_training_cluster
          
          libraries:
            - pypi:
                package: xgboost>=2.0.0
            - pypi:
                package: mlflow>=2.9.0
            - pypi:
                package: numpy>=1.24.0
            - pypi:
                package: pandas>=2.0.0
          
          spark_python_task:
            python_file: src/tree_models.py
            parameters:
              - --model-type
              - xgboost
              - --catalog
              - ${var.catalog}
              - --schema
              - ${var.schema}
              - --experiment-name
              - ${var.experiment_path}/xgboost
              - --run-name
              - xgboost-${bundle.target}
          
          timeout_seconds: 3600  # 1 hour
        
        # ========================================
        # Task 4: Model Evaluation
        # ========================================
        - task_key: model_evaluation
          depends_on:
            - task_key: train_catboost
            - task_key: train_xgboost
          
          job_cluster_key: ml_training_cluster
          
          libraries:
            - pypi:
                package: mlflow>=2.9.0
            - pypi:
                package: scikit-learn>=1.3.0
          
          spark_python_task:
            python_file: src/model_evaluation.py
            parameters:
              - --catalog
              - ${var.catalog}
              - --schema
              - ${var.schema}
              - --experiment-path
              - ${var.experiment_path}
              - --output-table
              - ${var.catalog}.${var.schema}.model_metrics
          
          timeout_seconds: 900  # 15 minutes
        
        # ========================================
        # Task 5: Fairness Analysis
        # ========================================
        - task_key: fairness_analysis
          depends_on:
            - task_key: model_evaluation
          
          job_cluster_key: ml_training_cluster
          
          libraries:
            - pypi:
                package: scipy>=1.11.0
            - pypi:
                package: numpy>=1.24.0
            - pypi:
                package: pandas>=2.0.0
          
          spark_python_task:
            python_file: src/fairness_analysis.py
            parameters:
              - --catalog
              - ${var.catalog}
              - --schema
              - ${var.schema}
              - --protected-attributes
              - patient_gender,geographic_region,patient_age_group
              - --threshold-percentile
              - "95.0"
              - --output-table
              - ${var.catalog}.${var.schema}.fairness_results
          
          timeout_seconds: 900  # 15 minutes
        
        # ========================================
        # Task 6: PSI Monitoring (Drift Detection)
        # ========================================
        - task_key: psi_monitoring
          depends_on:
            - task_key: model_evaluation
          
          job_cluster_key: ml_training_cluster
          
          libraries:
            - pypi:
                package: scipy>=1.11.0
            - pypi:
                package: numpy>=1.24.0
          
          spark_python_task:
            python_file: src/psi_monitoring.py
            parameters:
              - --catalog
              - ${var.catalog}
              - --schema
              - ${var.schema}
              - --reference-table
              - ${var.catalog}.${var.schema}.training_data
              - --current-table
              - ${var.catalog}.${var.schema}.features
              - --alert-threshold
              - "0.2"
              - --output-table
              - ${var.catalog}.${var.schema}.psi_results
          
          timeout_seconds: 600  # 10 minutes
        
        # ========================================
        # Task 7: Model Registration
        # ========================================
        - task_key: model_registration
          depends_on:
            - task_key: fairness_analysis
            - task_key: psi_monitoring
          
          job_cluster_key: ml_training_cluster
          
          libraries:
            - pypi:
                package: mlflow>=2.9.0
          
          spark_python_task:
            python_file: src/model_registration.py
            parameters:
              - --catalog
              - ${var.catalog}
              - --schema
              - ${var.schema}
              - --experiment-path
              - ${var.experiment_path}
              - --model-name
              - ${var.model_name}
              - --stage
              - ${bundle.target}
              - --require-fairness-validation
              - "true"
          
          timeout_seconds: 600  # 10 minutes
      
      # Job clusters configuration
      job_clusters:
        # Data processing cluster (smaller)
        - job_cluster_key: data_processing_cluster
          new_cluster:
            cluster_name: ""
            spark_version: ${var.cluster_spark_version}
            node_type_id: ${var.cluster_node_type}
            num_workers: ${var.cluster_min_workers}
            
            autoscale:
              min_workers: ${var.cluster_min_workers}
              max_workers: ${var.cluster_max_workers}
            
            spark_conf:
              spark.databricks.delta.preview.enabled: "true"
              spark.sql.adaptive.enabled: "true"
              spark.sql.adaptive.coalescePartitions.enabled: "true"
            
            autotermination_minutes: 10
            
            # Cost optimization
            aws_attributes:
              first_on_demand: 1
              availability: SPOT_WITH_FALLBACK
              zone_id: auto
              spot_bid_price_percent: 100
        
        # ML training cluster (larger, GPU optional for dev)
        - job_cluster_key: ml_training_cluster
          new_cluster:
            cluster_name: ""
            spark_version: ${var.cluster_spark_version}
            node_type_id: ${var.cluster_node_type}
            num_workers: ${var.cluster_min_workers}
            
            autoscale:
              min_workers: ${var.cluster_min_workers}
              max_workers: ${var.cluster_max_workers}
            
            spark_conf:
              spark.databricks.delta.preview.enabled: "true"
              spark.sql.adaptive.enabled: "true"
              spark.databricks.io.cache.enabled: "true"
            
            autotermination_minutes: 15
            
            # Cost optimization
            aws_attributes:
              first_on_demand: 1
              availability: SPOT_WITH_FALLBACK
              zone_id: auto
              spot_bid_price_percent: 100
      
      # Schedule (daily at 2 AM for prod/staging)
      schedule:
        quartz_cron_expression: "0 0 2 * * ?"  # Daily at 2 AM
        timezone_id: "America/Los_Angeles"
        pause_status: ${bundle.target == "dev" ? "PAUSED" : "UNPAUSED"}
      
      # Tags for organization
      tags:
        Environment: ${bundle.target}
        Project: claims-fraud-detection
        Team: ml-engineering
        Purpose: model-training
