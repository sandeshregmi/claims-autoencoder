# Claims Autoencoder Configuration

# Data Configuration
data:
  train_path: "data/claims_train.parquet"
  val_path: null  # Set to null to auto-split from train_path
  test_path: null  # Set to null to auto-split from train_path
  
  # Feature columns
  numerical_features:
    - claim_amount
    - patient_age
    - provider_experience_years
    - days_since_last_claim
    - num_previous_claims
    - average_claim_amount
    - claim_duration_days
    
  categorical_features:
    - claim_type
    - provider_specialty
    - diagnosis_code
    - procedure_code
    - patient_gender
    - geographic_region
    
  # Data processing
  handle_missing: "median"  # Options: "median", "mean", "drop", "forward_fill"
  outlier_treatment:
    enabled: true
    method: "iqr"  # Options: "iqr", "zscore"
    threshold: 3.0
  
  # Feature engineering
  feature_interactions:
    enabled: true
    pairs:
      - [claim_amount, patient_age]
      - [num_previous_claims, average_claim_amount]
  
  # Train/Val/Test split ratios (if single file provided)
  split_ratios:
    train: 0.7
    val: 0.15
    test: 0.15

# Model Architecture
model:
  encoding_dim: 32
  hidden_layers: [128, 64]  # Encoder layers (decoder is symmetric)
  activation: "relu"  # Options: "relu", "tanh", "leaky_relu"
  dropout_rate: 0.3
  batch_norm: true
  
  # Anomaly detection
  anomaly_threshold_percentile: 95  # Claims above this reconstruction error are anomalous

# Training Configuration
training:
  # Optimization
  batch_size: 256
  learning_rate: 0.001
  weight_decay: 0.0001
  optimizer: "adam"  # Options: "adam", "sgd", "adamw"
  
  # Learning rate scheduling
  lr_scheduler:
    enabled: true
    type: "reduce_on_plateau"  # Options: "reduce_on_plateau", "cosine", "step"
    patience: 10
    factor: 0.5
    min_lr: 0.00001
  
  # Training process
  max_epochs: 100
  early_stopping:
    enabled: true
    patience: 15
    min_delta: 0.001
    monitor: "val_loss"
  
  # Regularization
  gradient_clip_val: 1.0
  
  # Reproducibility
  seed: 42
  deterministic: true
  
  # Hardware
  accelerator: "auto"  # Options: "auto", "gpu", "cpu", "mps"
  devices: 1
  precision: "32"  # Options: "32", "16-mixed", "bf16-mixed"

# MLflow Configuration
mlflow:
  enabled: true
  tracking_uri: "mlruns"
  experiment_name: "claims_autoencoder"
  run_name: null  # Auto-generated if null
  
  # Logging
  log_params: true
  log_metrics: true
  log_artifacts: true
  log_model: true
  
  # Tags
  tags:
    project: "fraud_detection"
    team: "data_science"
    environment: "development"

# Evaluation Configuration
evaluation:
  metrics:
    - reconstruction_error
    - anomaly_detection_rate
    - precision_at_k
    - recall_at_k
  
  # Anomaly detection
  k_values: [10, 50, 100]  # For precision/recall at K
  
  # Visualization
  plot_distributions: true
  plot_roc_curve: true
  save_plots: true
  plots_dir: "outputs/plots"

# Monitoring Configuration
monitoring:
  psi:
    enabled: true
    num_bins: 10
    thresholds:
      minor: 0.1
      major: 0.2
    
  drift_detection:
    enabled: true
    check_frequency: "daily"  # Options: "daily", "weekly", "monthly"
    alert_threshold: 0.2

# Batch Scoring Configuration
batch_scoring:
  chunk_size: 10000
  num_workers: 4
  save_reconstructions: false
  output_format: "parquet"  # Options: "parquet", "csv"

# Hyperparameter Tuning Configuration
hyperparameter_tuning:
  enabled: false
  n_trials: 50
  timeout: 3600  # seconds
  
  # Parameter search space
  search_space:
    encoding_dim: [16, 32, 64, 128]
    hidden_layers:
      - [64]
      - [128, 64]
      - [256, 128, 64]
    dropout_rate: [0.1, 0.2, 0.3, 0.4, 0.5]
    learning_rate: [0.0001, 0.001, 0.01]
    batch_size: [128, 256, 512]
  
  # Optimization
  direction: "minimize"
  metric: "val_loss"
  pruner: "median"  # Options: "median", "hyperband", null

# Logging Configuration
logging:
  level: "INFO"  # Options: "DEBUG", "INFO", "WARNING", "ERROR"
  log_file: "logs/training.log"
  log_to_console: true
  log_to_file: true

# Paths Configuration
paths:
  data_dir: "data"
  models_dir: "models"
  outputs_dir: "outputs"
  logs_dir: "logs"
  checkpoints_dir: "checkpoints"

# Feature Store Configuration (Optional)
feature_store:
  enabled: false
  backend: "local"  # Options: "local", "feast", "sagemaker"
  cache_ttl: 3600  # seconds

# Tree Models Configuration (for fraud detection baselines)
tree_models:
  enabled: false  # Enable tree-based fraud detection
  types: ["xgboost", "catboost"]  # Models to train
  
  xgboost:
    max_depth: 6
    learning_rate: 0.1
    n_estimators: 100
    subsample: 0.8
    colsample_bytree: 0.8
    random_state: 42
  
  catboost:
    iterations: 100
    depth: 6
    learning_rate: 0.1
    random_seed: 42
    verbose: false

# Ensemble Configuration (combine neural and tree models)
ensemble:
  enabled: false  # Enable ensemble fraud detection
  method: "l2"  # Scoring method: l2, l1, or max
  weights:
    autoencoder: 0.5  # Neural autoencoder weight
    xgboost: 0.25     # XGBoost weight
    catboost: 0.25    # CatBoost weight
