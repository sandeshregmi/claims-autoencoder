# ═══════════════════════════════════════════════════════════════════════════
# ENHANCED CLAIMS FRAUD DETECTION CONFIGURATION
# Databricks-Ready | Fully Automated | 100% Config-Driven
# Version: 2.0
# ═══════════════════════════════════════════════════════════════════════════

# ═══════════════════════════════════════════════════════════════════════════
# DATABRICKS CONFIGURATION (NEW!)
# ═══════════════════════════════════════════════════════════════════════════
databricks:
  # Data source configuration
  data_source:
    type: "unity_catalog"  # Options: unity_catalog, delta_table, dbfs, external
    
    # Unity Catalog configuration
    catalog: "machine_learning"  # Your catalog name
    schema: "casetracker"        # Your schema name
    
    # Table/view names
    tables:
      claims: "claims"              # Main claims table
      patients: "patients"          # Patient demographics
      providers: "providers"        # Provider information
      fraud_labels: "fraud_labels"  # Historical fraud labels (if available)
    
    # SQL query template (supports parameterization)
    query_template: |
      SELECT 
        c.*,
        p.patient_age,
        p.patient_gender,
        p.geographic_region,
        pr.provider_specialty,
        pr.provider_experience_years
      FROM {catalog}.{schema}.{table} c
      LEFT JOIN {catalog}.{schema}.patients p ON c.patient_id = p.patient_id
      LEFT JOIN {catalog}.{schema}.providers pr ON c.provider_id = pr.provider_id
      WHERE c.claim_date >= '{start_date}'
        AND c.claim_date < '{end_date}'
        {additional_filters}
    
    # Column mapping (map Databricks columns to expected feature names)
    column_mapping:
      claim_amount: "claim_amt"            # Databricks col -> config name
      patient_age: "age"
      provider_experience_years: "provider_exp_yrs"
      claim_duration_days: "duration_days"
      num_previous_claims: "prev_claim_count"
      average_claim_amount: "avg_claim_amt"
      days_since_last_claim: "days_since_last"
      claim_type: "type"
      provider_specialty: "specialty"
      diagnosis_code: "diag_code"
      procedure_code: "proc_code"
      patient_gender: "gender"
      geographic_region: "region"
    
    # Data filters
    filters:
      date_range:
        enabled: true
        start_date: "2024-01-01"  # Override at runtime
        end_date: "2024-12-31"    # Override at runtime
      additional_conditions: ""   # e.g., "AND claim_status = 'APPROVED'"
  
  # Cluster configuration
  cluster:
    spark_version: "13.3.x-scala2.12"
    node_type_id: "i3.xlarge"
    num_workers: 2
    autoscale:
      min_workers: 1
      max_workers: 8
    spark_conf:
      "spark.databricks.delta.preview.enabled": "true"
      "spark.sql.adaptive.enabled": "true"
      "spark.sql.adaptive.coalescePartitions.enabled": "true"
  
  # Storage paths (DBFS or Unity Catalog Volumes)
  storage:
    use_volumes: true  # Use Unity Catalog Volumes (recommended)
    volume_path: "/Volumes/machine_learning/casetracker/claims_fraud"
    
    # Legacy DBFS paths (if not using volumes)
    dbfs_path: "/dbfs/FileStore/claims_fraud"
    
    # Paths within storage
    paths:
      data: "data"
      models: "models"
      checkpoints: "checkpoints"
      logs: "logs"
      outputs: "outputs"
  
  # Secrets management
  secrets:
    scope: "claims_fraud"  # Databricks secrets scope
    keys:
      database_password: "db_password"
      api_key: "api_key"
      mlflow_tracking_uri: "mlflow_uri"
  
  # MLflow on Databricks
  mlflow:
    use_databricks_tracking: true
    experiment_location: "/Shared/claims_fraud/experiments"
    model_registry_path: "models:/claims_fraud_autoencoder"
  
  # Jobs configuration
  jobs:
    training:
      name: "claims_fraud_training"
      schedule: "0 0 * * 0"  # Weekly on Sunday
      max_concurrent_runs: 1
      timeout_seconds: 7200
    
    batch_scoring:
      name: "claims_fraud_scoring"
      schedule: "0 2 * * *"  # Daily at 2 AM
      max_concurrent_runs: 1
      timeout_seconds: 3600
    
    monitoring:
      name: "claims_fraud_monitoring"
      schedule: "0 4 * * *"  # Daily at 4 AM
      max_concurrent_runs: 1
      timeout_seconds: 1800

# ═══════════════════════════════════════════════════════════════════════════
# DATA CONFIGURATION
# ═══════════════════════════════════════════════════════════════════════════
data:
  # Data source (local or Databricks)
  source: "databricks"  # Options: local, databricks, s3, azure, gcs
  
  # Local file paths (when source='local')
  train_path: "data/claims_train.parquet"
  val_path: null
  test_path: null
  
  # Features
  numerical_features:
    - claim_amount
    - patient_age
    - provider_experience_years
    - days_since_last_claim
    - num_previous_claims
    - average_claim_amount
    - claim_duration_days
    
  categorical_features:
    - claim_type
    - provider_specialty
    - diagnosis_code
    - procedure_code
    - patient_gender
    - geographic_region
  
  # Feature schemas
  feature_schemas:
    claim_amount:
      dtype: float
      min_value: 0
      max_value: 1000000
      default_value: 0
      required: true
      sql_type: "DECIMAL(10,2)"  # For Databricks schema
      
    patient_age:
      dtype: int
      min_value: 0
      max_value: 120
      default_value: 45
      required: true
      sql_type: "INT"
      
    provider_experience_years:
      dtype: int
      min_value: 0
      max_value: 50
      default_value: 10
      required: false
      sql_type: "INT"
      
    days_since_last_claim:
      dtype: float
      min_value: 0
      max_value: 3650
      default_value: 30
      required: false
      sql_type: "INT"
      
    num_previous_claims:
      dtype: int
      min_value: 0
      max_value: 100
      default_value: 0
      required: true
      sql_type: "INT"
      
    average_claim_amount:
      dtype: float
      min_value: 0
      max_value: 500000
      default_value: 5000
      required: false
      sql_type: "DECIMAL(10,2)"
      
    claim_duration_days:
      dtype: int
      min_value: 1
      max_value: 365
      default_value: 7
      required: true
      sql_type: "INT"
  
  # Categorical domains
  categorical_domains:
    claim_type:
      values: [medical, dental, vision, prescription, other]
      default: medical
      required: true
      sql_type: "STRING"
      
    provider_specialty:
      values: [general, specialist, surgeon, pediatrics, orthopedics, cardiology, neurology]
      default: general
      required: false
      sql_type: "STRING"
      
    diagnosis_code:
      values: [D001, D002, D003, D004, D005, D006, D007, D008, D009, D010]
      pattern: "D[0-9]{3}"
      default: D001
      required: true
      sql_type: "STRING"
      
    procedure_code:
      values: [P001, P002, P003, P004, P005]
      pattern: "P[0-9]{3}"
      default: P001
      required: true
      sql_type: "STRING"
      
    patient_gender:
      values: [M, F, O, Unknown]
      default: Unknown
      required: true
      sql_type: "STRING"
      
    geographic_region:
      values: [North, South, East, West, Central, Northeast, Southeast, Northwest, Southwest]
      default: Central
      required: false
      sql_type: "STRING"
  
  # Display names
  display_names:
    claim_amount: "Claim Amount ($)"
    patient_age: "Patient Age (years)"
    provider_experience_years: "Provider Experience (years)"
    days_since_last_claim: "Days Since Last Claim"
    num_previous_claims: "Number of Previous Claims"
    average_claim_amount: "Average Claim Amount ($)"
    claim_duration_days: "Claim Duration (days)"
    claim_type: "Claim Type"
    provider_specialty: "Provider Specialty"
    diagnosis_code: "Diagnosis Code"
    procedure_code: "Procedure Code"
    patient_gender: "Patient Gender"
    geographic_region: "Geographic Region"
  
  # Data quality
  data_quality:
    default_missing_rate: 0.05
    missing_patterns:
      claim_amount: 0.02
      patient_age: 0.01
      provider_experience_years: 0.05
      days_since_last_claim: 0.10
      num_previous_claims: 0.01
      average_claim_amount: 0.08
      claim_duration_days: 0.03
  
  # Imputation
  imputation:
    numerical:
      strategy: median
      fill_value: 0
    categorical:
      strategy: most_frequent
      fill_value: "Unknown"
  
  # Outlier treatment
  outlier_treatment:
    enabled: true
    method: iqr
    iqr:
      lower_quantile: 0.25
      upper_quantile: 0.75
      threshold_multiplier: 3.0
  
  # Scaling
  scaling:
    method: robust
    with_centering: true
    with_scaling: true
  
  # Categorical encoding
  categorical_encoding:
    method: label
    handle_unknown: most_frequent
  
  # Feature engineering
  feature_engineering:
    interactions:
      enabled: true
      pairs:
        - [claim_amount, patient_age]
        - [num_previous_claims, average_claim_amount]
    derived_features:
      - name: claim_per_age
        formula: "claim_amount / (patient_age + 1)"
        dtype: float
      - name: claim_frequency
        formula: "num_previous_claims / (days_since_last_claim + 1)"
        dtype: float
  
  # Split ratios
  split_ratios:
    train: 0.7
    val: 0.15
    test: 0.15

# ═══════════════════════════════════════════════════════════════════════════
# BUSINESS RULES
# ═══════════════════════════════════════════════════════════════════════════
business_rules:
  fraud_thresholds:
    claim_amount_high_risk: 100000
    claim_amount_medium_risk: 50000
    max_claims_per_month: 5
    max_claims_per_week: 2
    patient_age_min: 0
    patient_age_max: 120
    suspicious_age_ranges:
      - [0, 1]
      - [100, 120]
    
  alert_triggers:
    high_value_claim: 100000
    rapid_claims_count: 10
    
  validation_rules:
    max_claim_duration_days: 365
    min_claim_duration_days: 1
    require_diagnosis_code: true
    require_procedure_code: true

# ═══════════════════════════════════════════════════════════════════════════
# MODEL CONFIGURATION
# ═══════════════════════════════════════════════════════════════════════════
model:
  encoding_dim: 32
  hidden_layers: [128, 64]
  activation: relu
  dropout_rate: 0.3
  batch_norm: true
  anomaly_threshold_percentile: 95

# ═══════════════════════════════════════════════════════════════════════════
# TRAINING CONFIGURATION
# ═══════════════════════════════════════════════════════════════════════════
training:
  batch_size: 256
  learning_rate: 0.001
  weight_decay: 0.0001
  optimizer: adam
  max_epochs: 100
  seed: 42
  deterministic: true

# ═══════════════════════════════════════════════════════════════════════════
# MLFLOW CONFIGURATION
# ═══════════════════════════════════════════════════════════════════════════
mlflow:
  enabled: true
  tracking_uri: "databricks"  # Use Databricks MLflow
  experiment_name: "/Shared/claims_fraud/experiments/autoencoder"
  run_name: null

# ═══════════════════════════════════════════════════════════════════════════
# PATHS CONFIGURATION
# ═══════════════════════════════════════════════════════════════════════════
paths:
  data_dir: data
  models_dir: models
  outputs_dir: outputs
  logs_dir: logs
  checkpoints_dir: checkpoints

# ═══════════════════════════════════════════════════════════════════════════
# LOGGING CONFIGURATION
# ═══════════════════════════════════════════════════════════════════════════
logging:
  level: INFO
  log_file: logs/training.log
  log_to_console: true
  log_to_file: true
