# ðŸš€ Databricks + GitHub Actions Architecture

## Overview

This document outlines the production architecture for deploying the Claims Fraud Detection system as a Databricks Asset Bundle with GitHub Actions CI/CD.

## ðŸ—ï¸ Architecture Diagram

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                          GitHub Repository                           â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚  /src/                  - Python modules                       â”‚ â”‚
â”‚  â”‚  /databricks/          - Databricks configs                    â”‚ â”‚
â”‚  â”‚  /tests/               - Unit & integration tests              â”‚ â”‚
â”‚  â”‚  /.github/workflows/   - CI/CD pipelines                       â”‚ â”‚
â”‚  â”‚  /notebooks/           - Databricks notebooks                  â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                    â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚   GitHub Actions Trigger      â”‚
                    â”‚   (push, PR, schedule)        â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                    â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚                           â”‚                           â”‚
        â–¼                           â–¼                           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  CI Pipeline â”‚          â”‚Test Pipeline â”‚          â”‚ CD Pipeline  â”‚
â”‚              â”‚          â”‚              â”‚          â”‚              â”‚
â”‚ â€¢ Lint       â”‚          â”‚ â€¢ Unit Tests â”‚          â”‚ â€¢ Deploy DEV â”‚
â”‚ â€¢ Format     â”‚          â”‚ â€¢ Integrationâ”‚          â”‚ â€¢ Deploy STG â”‚
â”‚ â€¢ Security   â”‚          â”‚ â€¢ Fairness   â”‚          â”‚ â€¢ Deploy PRODâ”‚
â”‚ â€¢ Type Check â”‚          â”‚ â€¢ Drift      â”‚          â”‚ â€¢ Validate   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                    â”‚
                                    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        Databricks Workspace                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚                    Asset Bundle Deployment                      â”‚ â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚ â”‚
â”‚  â”‚  â”‚ DEV          â”‚  â”‚ STAGING      â”‚  â”‚ PRODUCTION   â”‚        â”‚ â”‚
â”‚  â”‚  â”‚              â”‚  â”‚              â”‚  â”‚              â”‚        â”‚ â”‚
â”‚  â”‚  â”‚ â€¢ Workflows  â”‚  â”‚ â€¢ Workflows  â”‚  â”‚ â€¢ Workflows  â”‚        â”‚ â”‚
â”‚  â”‚  â”‚ â€¢ Jobs       â”‚  â”‚ â€¢ Jobs       â”‚  â”‚ â€¢ Jobs       â”‚        â”‚ â”‚
â”‚  â”‚  â”‚ â€¢ Models     â”‚  â”‚ â€¢ Models     â”‚  â”‚ â€¢ Models     â”‚        â”‚ â”‚
â”‚  â”‚  â”‚ â€¢ MLflow     â”‚  â”‚ â€¢ MLflow     â”‚  â”‚ â€¢ MLflow     â”‚        â”‚ â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                                                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚                     Data Processing Layer                       â”‚ â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚ â”‚
â”‚  â”‚  â”‚ Delta Tables â”‚  â”‚ Feature Storeâ”‚  â”‚ Model Registryâ”‚       â”‚ â”‚
â”‚  â”‚  â”‚              â”‚  â”‚              â”‚  â”‚              â”‚        â”‚ â”‚
â”‚  â”‚  â”‚ â€¢ Raw Data   â”‚  â”‚ â€¢ Features   â”‚  â”‚ â€¢ Models     â”‚        â”‚ â”‚
â”‚  â”‚  â”‚ â€¢ Processed  â”‚  â”‚ â€¢ Metadata   â”‚  â”‚ â€¢ Versions   â”‚        â”‚ â”‚
â”‚  â”‚  â”‚ â€¢ Curated    â”‚  â”‚ â€¢ Lineage    â”‚  â”‚ â€¢ Stages     â”‚        â”‚ â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                                                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚                     ML Pipeline Jobs                            â”‚ â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚ â”‚
â”‚  â”‚  â”‚ Training     â”‚  â”‚ Batch Scoringâ”‚  â”‚ Monitoring   â”‚        â”‚ â”‚
â”‚  â”‚  â”‚              â”‚  â”‚              â”‚  â”‚              â”‚        â”‚ â”‚
â”‚  â”‚  â”‚ â€¢ Autoencoderâ”‚  â”‚ â€¢ Claims     â”‚  â”‚ â€¢ PSI        â”‚        â”‚ â”‚
â”‚  â”‚  â”‚ â€¢ XGBoost    â”‚  â”‚ â€¢ Fraud Flagsâ”‚  â”‚ â€¢ Fairness   â”‚        â”‚ â”‚
â”‚  â”‚  â”‚ â€¢ CatBoost   â”‚  â”‚ â€¢ Scores     â”‚  â”‚ â€¢ Drift      â”‚        â”‚ â”‚
â”‚  â”‚  â”‚ â€¢ TabNet     â”‚  â”‚ â€¢ Export     â”‚  â”‚ â€¢ Alerts     â”‚        â”‚ â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ðŸ“ Project Structure for Databricks

```
claims-autoencoder/
â”œâ”€â”€ databricks.yml                    # Asset Bundle root config
â”œâ”€â”€ .github/
â”‚   â””â”€â”€ workflows/
â”‚       â”œâ”€â”€ ci.yml                    # Continuous Integration
â”‚       â”œâ”€â”€ test.yml                  # Testing pipeline
â”‚       â”œâ”€â”€ deploy-dev.yml            # Deploy to DEV
â”‚       â”œâ”€â”€ deploy-staging.yml        # Deploy to STAGING
â”‚       â””â”€â”€ deploy-prod.yml           # Deploy to PROD
â”œâ”€â”€ databricks/
â”‚   â”œâ”€â”€ bundle/
â”‚   â”‚   â”œâ”€â”€ dev/
â”‚   â”‚   â”‚   â””â”€â”€ databricks.yml        # DEV environment config
â”‚   â”‚   â”œâ”€â”€ staging/
â”‚   â”‚   â”‚   â””â”€â”€ databricks.yml        # STAGING environment config
â”‚   â”‚   â””â”€â”€ prod/
â”‚   â”‚       â””â”€â”€ databricks.yml        # PROD environment config
â”‚   â”œâ”€â”€ workflows/
â”‚   â”‚   â”œâ”€â”€ training_pipeline.yml     # Model training workflow
â”‚   â”‚   â”œâ”€â”€ batch_scoring.yml         # Batch inference workflow
â”‚   â”‚   â”œâ”€â”€ monitoring.yml            # Monitoring & alerting workflow
â”‚   â”‚   â””â”€â”€ fairness_audit.yml        # Fairness analysis workflow
â”‚   â””â”€â”€ jobs/
â”‚       â”œâ”€â”€ train_models.py           # Training job
â”‚       â”œâ”€â”€ score_claims.py           # Scoring job
â”‚       â”œâ”€â”€ monitor_drift.py          # PSI monitoring job
â”‚       â””â”€â”€ fairness_check.py         # Fairness validation job
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ config_manager.py
â”‚   â”œâ”€â”€ data_ingestion.py
â”‚   â”œâ”€â”€ tree_models.py
â”‚   â”œâ”€â”€ fairness_analysis.py
â”‚   â”œâ”€â”€ psi_monitoring.py
â”‚   â””â”€â”€ coordination_hooks.py
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ 01_exploratory_analysis.py    # Databricks notebook
â”‚   â”œâ”€â”€ 02_model_development.py
â”‚   â”œâ”€â”€ 03_fairness_validation.py
â”‚   â””â”€â”€ 04_deployment_validation.py
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ unit/
â”‚   â”‚   â”œâ”€â”€ test_models.py
â”‚   â”‚   â”œâ”€â”€ test_fairness.py
â”‚   â”‚   â””â”€â”€ test_monitoring.py
â”‚   â”œâ”€â”€ integration/
â”‚   â”‚   â”œâ”€â”€ test_pipeline.py
â”‚   â”‚   â””â”€â”€ test_workflows.py
â”‚   â””â”€â”€ fixtures/
â”‚       â””â”€â”€ sample_data.parquet
â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ dev_config.yaml
â”‚   â”œâ”€â”€ staging_config.yaml
â”‚   â””â”€â”€ prod_config.yaml
â””â”€â”€ requirements.txt
```

## ðŸ”§ Databricks Asset Bundle Configuration

### Root `databricks.yml`

```yaml
bundle:
  name: claims-fraud-detection
  
# Include environment-specific configs
include:
  - resources/*.yml
  
# Sync configuration
sync:
  include:
    - src/**
    - config/**
    - notebooks/**
  exclude:
    - tests/**
    - .git/**
    - __pycache__/**
    
# Variables that can be overridden per environment
variables:
  warehouse_id:
    description: SQL Warehouse ID for querying
    default: ${var.sql_warehouse_id}
  
  catalog:
    description: Unity Catalog name
    default: ${var.catalog_name}
  
  schema:
    description: Schema name for tables
    default: fraud_detection
    
  cluster_config:
    description: Cluster configuration
    default:
      num_workers: 2
      node_type_id: i3.xlarge
      spark_version: 14.3.x-scala2.12
```

### Environment Config: `databricks/bundle/prod/databricks.yml`

```yaml
bundle:
  name: claims-fraud-detection
  
targets:
  prod:
    mode: production
    
    # Production workspace
    workspace:
      host: https://your-workspace.cloud.databricks.com
      
    # Permissions
    permissions:
      - level: CAN_MANAGE
        group_name: ml-engineers
      - level: CAN_VIEW
        group_name: data-scientists
    
    # Variable overrides for production
    variables:
      catalog: production
      warehouse_id: ${secrets/prod/sql_warehouse_id}
      cluster_config:
        num_workers: 4
        node_type_id: i3.2xlarge
        autoscale:
          min_workers: 2
          max_workers: 8

resources:
  jobs:
    # Daily training job
    model_training:
      name: "[PROD] Claims Fraud - Model Training"
      
      tasks:
        - task_key: train_autoencoder
          notebook_task:
            notebook_path: /Workspace/claims-fraud-detection/notebooks/02_model_development
            base_parameters:
              model_type: catboost
              config_path: config/prod_config.yaml
          
          new_cluster:
            num_workers: ${var.cluster_config.num_workers}
            node_type_id: ${var.cluster_config.node_type_id}
            spark_version: ${var.cluster_config.spark_version}
            spark_conf:
              spark.databricks.delta.preview.enabled: true
          
          libraries:
            - pypi:
                package: catboost
            - pypi:
                package: xgboost
            - pypi:
                package: shap
                
        - task_key: fairness_validation
          depends_on:
            - task_key: train_autoencoder
          
          python_wheel_task:
            package_name: fraud_detection
            entry_point: fairness_check
            parameters:
              - --model-path
              - ${tasks.train_autoencoder.outputs.model_path}
              - --threshold
              - "95.0"
          
          libraries:
            - whl: /Workspace/claims-fraud-detection/dist/fraud_detection-0.1.0-py3-none-any.whl
      
      schedule:
        quartz_cron_expression: "0 0 2 * * ?"  # Daily at 2 AM
        timezone_id: America/Los_Angeles
      
      email_notifications:
        on_failure:
          - ml-team@company.com
        on_success:
          - ml-team@company.com
      
      tags:
        environment: production
        team: ml-engineering
    
    # Batch scoring job
    batch_scoring:
      name: "[PROD] Claims Fraud - Batch Scoring"
      
      tasks:
        - task_key: score_claims
          spark_python_task:
            python_file: /Workspace/claims-fraud-detection/databricks/jobs/score_claims.py
            parameters:
              - --input-table
              - ${var.catalog}.${var.schema}.claims_raw
              - --output-table
              - ${var.catalog}.${var.schema}.fraud_scores
              - --model-version
              - production
          
          existing_cluster_id: ${var.scoring_cluster_id}
          
          libraries:
            - pypi:
                package: mlflow
      
      schedule:
        quartz_cron_expression: "0 */6 * * * ?"  # Every 6 hours
        timezone_id: America/Los_Angeles
      
      tags:
        environment: production
        purpose: scoring
    
    # Monitoring job
    drift_monitoring:
      name: "[PROD] Claims Fraud - Drift Monitoring"
      
      tasks:
        - task_key: psi_check
          python_wheel_task:
            package_name: fraud_detection
            entry_point: monitor_drift
            parameters:
              - --reference-table
              - ${var.catalog}.${var.schema}.training_data
              - --current-table
              - ${var.catalog}.${var.schema}.claims_raw
              - --alert-threshold
              - "0.2"
          
          libraries:
            - whl: /Workspace/claims-fraud-detection/dist/fraud_detection-0.1.0-py3-none-any.whl
      
      schedule:
        quartz_cron_expression: "0 0 */4 * * ?"  # Every 4 hours
        timezone_id: America/Los_Angeles
      
      email_notifications:
        on_failure:
          - ml-team@company.com
        on_duration_warning_threshold_exceeded:
          - ml-team@company.com
      
      health:
        rules:
          - metric: RUN_DURATION_SECONDS
            op: GREATER_THAN
            value: 3600  # Alert if job runs longer than 1 hour
      
      tags:
        environment: production
        purpose: monitoring

  # Model serving endpoint
  model_serving_endpoints:
    fraud_detection_endpoint:
      name: fraud-detection-scoring
      
      config:
        served_models:
          - model_name: claims_fraud_autoencoder
            model_version: ${var.model_version}
            workload_size: Small
            scale_to_zero_enabled: false
            environment_vars:
              FAIRNESS_CHECK_ENABLED: "true"
              PSI_MONITORING_ENABLED: "true"
        
        auto_capture_config:
          enabled: true
          catalog_name: ${var.catalog}
          schema_name: ${var.schema}
          table_name_prefix: fraud_endpoint_logs
      
      tags:
        environment: production
        
  # Delta Live Tables pipeline for data processing
  pipelines:
    claims_processing:
      name: claims-data-processing
      
      catalog: ${var.catalog}
      target: ${var.schema}
      
      libraries:
        - notebook:
            path: /Workspace/claims-fraud-detection/notebooks/dlt_pipeline
      
      clusters:
        - label: default
          num_workers: 2
          node_type_id: i3.xlarge
      
      development: false
      continuous: true
      
      notifications:
        - email_recipients:
            - data-engineering@company.com
          alerts:
            - on-update-failure
            - on-update-fatal-failure
```

## ðŸ”„ GitHub Actions Workflows

### CI Pipeline: `.github/workflows/ci.yml`

```yaml
name: CI - Lint, Format, Type Check

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main, develop ]

jobs:
  ci:
    runs-on: ubuntu-latest
    
    steps:
      - uses: actions/checkout@v3
      
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.10'
          cache: 'pip'
      
      - name: Install dependencies
        run: |
          pip install -r requirements.txt
          pip install black flake8 mypy pytest
      
      - name: Black formatting check
        run: black --check src/ tests/
      
      - name: Flake8 linting
        run: flake8 src/ tests/ --max-line-length=120
      
      - name: MyPy type checking
        run: mypy src/ --ignore-missing-imports
      
      - name: Security check with bandit
        run: |
          pip install bandit
          bandit -r src/ -ll
```

### Test Pipeline: `.github/workflows/test.yml`

```yaml
name: Tests - Unit, Integration, Fairness

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main, develop ]

jobs:
  test:
    runs-on: ubuntu-latest
    
    steps:
      - uses: actions/checkout@v3
      
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.10'
          cache: 'pip'
      
      - name: Install dependencies
        run: |
          pip install -r requirements.txt
          pip install pytest pytest-cov
      
      - name: Run unit tests
        run: pytest tests/unit/ -v --cov=src --cov-report=xml
      
      - name: Run integration tests
        run: pytest tests/integration/ -v
      
      - name: Test fairness analysis
        run: |
          python -m pytest tests/unit/test_fairness.py -v
          python -c "from src.fairness_analysis import FairnessAnalyzer; print('âœ… Fairness module OK')"
      
      - name: Test PSI monitoring
        run: |
          python -m pytest tests/unit/test_monitoring.py -v
          python -c "from src.psi_monitoring import PSIMonitor; print('âœ… PSI module OK')"
      
      - name: Upload coverage reports
        uses: codecov/codecov-action@v3
        with:
          file: ./coverage.xml
          flags: unittests
```

### Deploy to DEV: `.github/workflows/deploy-dev.yml`

```yaml
name: Deploy to DEV

on:
  push:
    branches: [ develop ]
  workflow_dispatch:

jobs:
  deploy-dev:
    runs-on: ubuntu-latest
    environment: dev
    
    steps:
      - uses: actions/checkout@v3
      
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.10'
      
      - name: Install Databricks CLI
        run: |
          pip install databricks-cli
      
      - name: Configure Databricks CLI
        run: |
          echo "[DEFAULT]" > ~/.databrickscfg
          echo "host = ${{ secrets.DATABRICKS_HOST_DEV }}" >> ~/.databrickscfg
          echo "token = ${{ secrets.DATABRICKS_TOKEN_DEV }}" >> ~/.databrickscfg
      
      - name: Validate bundle
        run: |
          databricks bundle validate -t dev
      
      - name: Deploy to DEV
        run: |
          databricks bundle deploy -t dev
      
      - name: Run smoke tests
        run: |
          databricks bundle run smoke_test_job -t dev
      
      - name: Notify deployment
        uses: 8398a7/action-slack@v3
        with:
          status: ${{ job.status }}
          text: 'DEV deployment completed'
          webhook_url: ${{ secrets.SLACK_WEBHOOK }}
```

### Deploy to PROD: `.github/workflows/deploy-prod.yml`

```yaml
name: Deploy to PRODUCTION

on:
  release:
    types: [published]
  workflow_dispatch:
    inputs:
      version:
        description: 'Version to deploy'
        required: true

jobs:
  deploy-staging:
    runs-on: ubuntu-latest
    environment: staging
    
    steps:
      - uses: actions/checkout@v3
        with:
          ref: ${{ github.event.inputs.version || github.ref }}
      
      - name: Deploy to STAGING
        run: |
          databricks bundle deploy -t staging
      
      - name: Run validation tests
        run: |
          databricks bundle run validation_tests -t staging
      
      - name: Fairness validation
        run: |
          python databricks/jobs/fairness_check.py \
            --environment staging \
            --threshold 95.0 \
            --max-di-ratio 1.25
      
      - name: PSI validation
        run: |
          python databricks/jobs/monitor_drift.py \
            --environment staging \
            --alert-threshold 0.2

  deploy-production:
    needs: deploy-staging
    runs-on: ubuntu-latest
    environment:
      name: production
      url: https://your-workspace.cloud.databricks.com
    
    steps:
      - uses: actions/checkout@v3
      
      - name: Deploy to PRODUCTION
        run: |
          databricks bundle deploy -t prod
      
      - name: Smoke tests
        run: |
          databricks bundle run smoke_test_job -t prod
      
      - name: Update model registry
        run: |
          python scripts/promote_model.py \
            --from-stage Staging \
            --to-stage Production \
            --model-name claims_fraud_autoencoder
      
      - name: Create deployment tag
        uses: actions/github-script@v6
        with:
          script: |
            github.rest.git.createRef({
              owner: context.repo.owner,
              repo: context.repo.repo,
              ref: 'refs/tags/prod-${{ github.run_number }}',
              sha: context.sha
            })
      
      - name: Notify success
        uses: 8398a7/action-slack@v3
        with:
          status: success
          text: 'ðŸš€ PROD deployment successful!'
          webhook_url: ${{ secrets.SLACK_WEBHOOK }}
```

## ðŸ“Š Key Databricks Jobs

### Training Job: `databricks/jobs/train_models.py`

```python
"""
Databricks training job with fairness validation
"""

import argparse
from pyspark.sql import SparkSession
import mlflow
from src.tree_models import ClaimsTreeAutoencoder
from src.fairness_analysis import FairnessAnalyzer
from src.coordination_hooks import coordination_hooks


def main():
    parser = argparse.ArgumentParser()
    parser.add_argument('--model-type', default='catboost')
    parser.add_argument('--config-path', required=True)
    parser.add_argument('--output-path', required=True)
    args = parser.parse_args()
    
    # Initialize Spark
    spark = SparkSession.builder.getOrCreate()
    
    # Start MLflow run
    with mlflow.start_run(run_name=f"training_{args.model_type}") as run:
        
        # Load data from Delta
        train_df = spark.table("fraud_detection.training_data").toPandas()
        
        # Pre-training hook
        coordination_hooks.pre_model_training(args.model_type)
        
        # Train model
        model = ClaimsTreeAutoencoder(model_type=args.model_type)
        model.fit(train_df, cat_features, num_features)
        
        # Compute scores
        fraud_scores, _ = model.compute_fraud_scores(train_df)
        
        # Post-training hook
        coordination_hooks.post_model_training(args.model_type, {
            "mean_score": float(fraud_scores.mean()),
            "p95_score": float(np.percentile(fraud_scores, 95))
        })
        
        # Fairness validation
        coordination_hooks.pre_fairness_analysis(['patient_gender', 'geographic_region'])
        
        analyzer = FairnessAnalyzer(
            data=train_df,
            fraud_scores=fraud_scores,
            protected_attributes=['patient_gender', 'geographic_region']
        )
        
        fairness_results = analyzer.analyze_all_attributes()
        coordination_hooks.post_fairness_analysis(fairness_results)
        
        # Log to MLflow
        mlflow.log_params({
            "model_type": args.model_type,
            "n_samples": len(train_df)
        })
        
        mlflow.log_metrics({
            "mean_fraud_score": float(fraud_scores.mean()),
            "p95_fraud_score": float(np.percentile(fraud_scores, 95))
        })
        
        # Check fairness before logging model
        bias_detected = any(
            not result.get('overall_metrics', {}).get('is_fair', True)
            for result in fairness_results.values()
        )
        
        mlflow.log_metric("bias_detected", int(bias_detected))
        
        if bias_detected:
            mlflow.set_tag("fairness_warning", "Bias detected in protected attributes")
        
        # Log model
        mlflow.sklearn.log_model(model, "model")
        
        # Save to Delta
        spark.createDataFrame(train_df).write.format("delta").mode("overwrite").saveAsTable(args.output_path)


if __name__ == "__main__":
    main()
```

### Monitoring Job: `databricks/jobs/monitor_drift.py`

```python
"""
Databricks PSI monitoring job with automated alerts
"""

import argparse
from pyspark.sql import SparkSession
from src.psi_monitoring import PSIMonitor
from src.coordination_hooks import coordination_hooks


def main():
    parser = argparse.ArgumentParser()
    parser.add_argument('--reference-table', required=True)
    parser.add_argument('--current-table', required=True)
    parser.add_argument('--alert-threshold', type=float, default=0.2)
    args = parser.parse_args()
    
    spark = SparkSession.builder.getOrCreate()
    
    # Load data
    reference_df = spark.table(args.reference_table).toPandas()
    current_df = spark.table(args.current_table).toPandas()
    
    # Pre-drift detection hook
    coordination_hooks.pre_drift_detection()
    
    # Initialize PSI monitor
    psi_monitor = PSIMonitor(
        reference_data=reference_df[num_features].values,
        num_bins=10,
        feature_names=num_features
    )
    
    # Detect drift
    psi_results = psi_monitor.detect_drift(current_df[num_features].values)
    
    # Post-drift detection hook (includes alerts)
    coordination_hooks.post_drift_detection(psi_results)
    
    # Trigger retraining if major drift
    if psi_results['drift_status'] == 'major':
        print("ðŸš¨ MAJOR DRIFT - Triggering retraining workflow")
        
        # Trigger training job via Databricks Jobs API
        from databricks.sdk import WorkspaceClient
        w = WorkspaceClient()
        
        w.jobs.run_now(
            job_id=TRAINING_JOB_ID,
            notebook_params={
                "trigger_reason": "major_drift_detected",
                "psi_score": str(psi_results['overall_psi'])
            }
        )


if __name__ == "__main__":
    main()
```

## ðŸŽ¯ Benefits of This Architecture

### 1. **Scalability**
- âœ… Databricks handles massive datasets
- âœ… Auto-scaling clusters
- âœ… Delta Lake for ACID transactions
- âœ… Unity Catalog for governance

### 2. **CI/CD Best Practices**
- âœ… Automated testing on every commit
- âœ… Environment promotion (DEV â†’ STAGING â†’ PROD)
- âœ… Automated fairness & drift checks
- âœ… Rollback capabilities

### 3. **MLOps Integration**
- âœ… MLflow experiment tracking
- âœ… Model registry with stages
- âœ… Model serving endpoints
- âœ… Automated monitoring

### 4. **Governance & Compliance**
- âœ… Fairness validation in CI/CD
- âœ… Audit trail of all deployments
- âœ… Role-based access control
- âœ… Data lineage tracking

### 5. **Cost Optimization**
- âœ… Auto-scaling based on load
- âœ… Spot instances for development
- âœ… Scheduled job optimization
- âœ… Resource monitoring

## ðŸ“ˆ Recommended Implementation Timeline

### Week 1: Setup & Migration
- [ ] Create Databricks workspace
- [ ] Setup GitHub repository structure
- [ ] Configure Unity Catalog
- [ ] Migrate code to Databricks format

### Week 2: CI/CD Pipeline
- [ ] Setup GitHub Actions
- [ ] Create DEV environment
- [ ] Configure automated testing
- [ ] Setup deployment workflows

### Week 3: Data Pipeline
- [ ] Create Delta tables
- [ ] Setup Delta Live Tables
- [ ] Configure Feature Store
- [ ] Test data ingestion

### Week 4: ML Pipeline
- [ ] Deploy training jobs
- [ ] Setup MLflow tracking
- [ ] Configure model registry
- [ ] Test batch scoring

### Week 5: Monitoring & Fairness
- [ ] Deploy PSI monitoring
- [ ] Setup fairness validation
- [ ] Configure alerting
- [ ] Create dashboards

### Week 6: Production Deployment
- [ ] Create STAGING environment
- [ ] Deploy to STAGING
- [ ] Validate all workflows
- [ ] Deploy to PRODUCTION

## ðŸš€ Next Steps

1. **Review this architecture** - Make sure it aligns with your organization's standards
2. **Setup Databricks workspace** - Get access to Databricks
3. **Create GitHub repository** - Setup the repo structure
4. **Start with DEV environment** - Deploy to DEV first
5. **Iterate and improve** - Continuously enhance based on feedback

Want me to create any specific component (e.g., Delta Live Tables pipeline, specific GitHub Action, or Databricks job)? ðŸš€
